# -*- coding: utf-8 -*-
"""MWE_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16OMsNLSRYG2arlNjYEOTkUrV75dq1-FU
"""

# function that converts cupt file into bio labels
def parseme_cupt_to_bio(parseme_file):
    sentences = []
    tokens, labels = [], []
    active_mwes = set()  # keeps track of already started MWEs

    with open(parseme_file, encoding="utf-8") as f:
        for line in f:
            line = line.strip()

            if not line or line.startswith("#"):
                if tokens:
                    sentences.append({"tokens": tokens, "labels": labels})
                    tokens, labels, active_mwes = [], [], set()
                continue

            cols = line.split("\t")
            if len(cols) < 2:
                continue

            token = cols[1]
            mwe_col = cols[-1]

            label = "O"
            if mwe_col not in {"_", "*"}:
                for mwe in mwe_col.split(";"):
                    if ":" in mwe:
                        mwe_id, _ = mwe.split(":", 1)
                        if mwe_id not in active_mwes:
                            label = "B-MWE"
                            active_mwes.add(mwe_id)
                        else:
                            label = "I-MWE"
                    else:
                        if mwe in active_mwes:
                            label = "I-MWE"

            tokens.append(token)
            labels.append(label)

    return sentences

# checks that the function "parseme_cupt_to_bio" works properly
from collections import Counter

train_sents = parseme_cupt_to_bio("train.cupt")
all_labels = [lab for sent in train_sents for lab in sent["labels"]]
print(Counter(all_labels))

from datasets import Dataset, DatasetDict

# uploads files already converted by the function
train_sents = parseme_cupt_to_bio("train.cupt")

# creates a dataset
dataset = DatasetDict({
    "train": Dataset.from_list(train_sents)
})

print(dataset)

# filters train dataset with only sentences containing MWEs
def filter_with_mwe(sents):
    return [s for s in sents if any(l != "O" for l in s["labels"])]

train_sents_mwe = filter_with_mwe(train_sents)

print("Original train size:", len(train_sents))
print("Filtered train size:", len(train_sents_mwe))

from datasets import Dataset, DatasetDict

# splits data into train 80%, evaluation (dev) 10%, and test 10%
n = len(train_sents_mwe)
train_split = int(n * 0.8)
dev_split   = int(n * 0.9)

dataset = DatasetDict({
    "train": Dataset.from_list(train_sents_mwe[:train_split]),
    "dev": Dataset.from_list(train_sents_mwe[train_split:dev_split]),
    "test": Dataset.from_list(train_sents_mwe[dev_split:])
})

print(dataset)

# maps each label to a numerical id for later use
from collections import Counter

# list of labels in the dataset
unique_labels = sorted(set(lab for sent in train_sents for lab in sent["labels"]))
label2id = {label: i for i, label in enumerate(unique_labels)}
id2label = {i: label for label, i in label2id.items()}

print("Label2ID:", label2id)
print("ID2Label:", id2label)

def convert_labels_to_ids(examples):
    new_labels = []
    for lbls in examples["labels"]:
        new_labels.append([label2id[l] for l in lbls])
    examples["labels"] = new_labels
    return examples

dataset = dataset.map(convert_labels_to_ids, batched=True)

# we tokenize and align with subwords
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-italian-cased")

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"],
        is_split_into_words=True,
        truncation=True,
        padding="max_length",
        max_length=128,
    )

    all_labels = []
    for i, labels in enumerate(examples["labels"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # ignorato dal loss
            elif word_idx != previous_word_idx:
                label_ids.append(labels[word_idx])
            else:
                if labels[word_idx] == label2id["B-MWE"]:
                    label_ids.append(label2id["I-MWE"])
                else:
                    label_ids.append(labels[word_idx])
            previous_word_idx = word_idx
        all_labels.append(label_ids)

    tokenized_inputs["labels"] = all_labels
    return tokenized_inputs

tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)
print(tokenized_dataset)

!pip install seqeval

import torch
import numpy as np
from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer
from seqeval.metrics import classification_report, f1_score

# check GPU: the model should run on GPU
print("GPU available:", torch.cuda.is_available())

# creates the model from a pre-trained bert model
num_labels = len(label2id)

model = AutoModelForTokenClassification.from_pretrained(
    "dbmdz/bert-base-italian-cased",
    num_labels=num_labels,
    id2label=id2label,
    label2id=label2id
)

# set preferred training variables
training_args = TrainingArguments(
    output_dir="./mwe_bert",
    learning_rate=3e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    num_train_epochs=5,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=20,
    save_total_limit=1,
)

# set the evaluation of the model with true labels, true predictions and f1 score
def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    true_labels = [
        [id2label[l] for (l, lab) in zip(label_row, label_row_orig) if lab != -100]
        for label_row, label_row_orig in zip(predictions, labels)
    ]
    true_preds = [
        [id2label[pred] for (pred, lab) in zip(pred_row, label_row) if lab != -100]
        for pred_row, label_row in zip(predictions, labels)
    ]

    f1 = f1_score(true_labels, true_preds)
    report = classification_report(true_labels, true_preds)
    return {"f1": f1, "report": report}

# set the trainer datasets and the other necessary variables
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["dev"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# trains the model
# if needed, API key is: e84cd7f639d3bf50b4a74761d5ea1fde28a455af
trainer.train()

# checks statistics about performance
metrics = trainer.evaluate()
print(metrics)

# saves the trained model and tokenizer
trainer.save_model("./mwe_bert")
tokenizer.save_pretrained("./mwe_bert")

print("Model and tokenizer saved in ./mwe_bert")

from transformers import pipeline

# loads the trained model
inference_pipeline = pipeline(
    "token-classification",
    model="./mwe_bert",
    tokenizer=tokenizer,
    aggregation_strategy="simple"
)

# takes the first three sentences from the evaluation dataset to test
for i in range(3):
    example = tokenized_dataset["dev"][i]
    tokens = tokenizer.convert_ids_to_tokens(example["input_ids"], skip_special_tokens=True)

    true_labels = [id2label[label] for label in example["labels"] if label != -100]

    sentence = tokenizer.decode(example["input_ids"], skip_special_tokens=True)

    # model predictions
    preds = inference_pipeline(sentence)

    print(f"\n--- Example {i+1} ---")
    print("Sentence:", sentence)
    print("True labels:", true_labels)
    print("Predictions:", preds)

# prediction outputs
pred_output = trainer.predict(tokenized_dataset["test"])
logits = pred_output.predictions
labels = pred_output.label_ids

pred_ids = logits.argmax(axis=-1)

y_true = []
y_pred = []
for lab_row, pred_row in zip(labels, pred_ids):
    true_row = []
    pred_row_labels = []
    for l, p in zip(lab_row, pred_row):
        if l == -100:
            continue
        true_row.append(id2label[int(l)])
        pred_row_labels.append(id2label[int(p)])
    y_true.append(true_row)
    y_pred.append(pred_row_labels)

# seqeval evaluation metrics
from seqeval.metrics import classification_report, f1_score, precision_score, recall_score
print("F1 (micro-like) :", f1_score(y_true, y_pred))
print(classification_report(y_true, y_pred))

def bio_spans(labels):
    """labels: list of BIO strings per token -> return list of (start, end) spans"""
    spans = []
    start = None
    for i, lab in enumerate(labels):
        if lab.startswith("B-"):
            if start is not None:
                spans.append((start, i-1))
            start = i
        elif lab.startswith("I-"):
            continue
        else:  # 'O'
            if start is not None:
                spans.append((start, i-1))
                start = None
    if start is not None:
        spans.append((start, len(labels)-1))
    return spans

# compute precision, recall and f1 score
tp = 0
pred_count = 0
gold_count = 0
for true_row, pred_row in zip(y_true, y_pred):
    gold_spans = set(bio_spans(true_row))
    pred_spans = set(bio_spans(pred_row))
    tp += len(gold_spans & pred_spans)
    pred_count += len(pred_spans)
    gold_count += len(gold_spans)

precision = tp / pred_count if pred_count else 0.0
recall = tp / gold_count if gold_count else 0.0
f1 = (2 * precision * recall / (precision + recall)) if (precision+recall)>0 else 0.0
print(f"Span-level P={precision:.4f} R={recall:.4f} F1={f1:.4f}")

# identifies and saves mismatches in the datasets
import csv
out = []
for i, (example, true_row, pred_row) in enumerate(zip(tokenized_dataset["test"], y_true, y_pred)):
    if true_row != pred_row:
        sent = tokenizer.decode(example["input_ids"], skip_special_tokens=True)
        out.append({
            "idx": i,
            "sentence": sent,
            "true": " ".join(true_row),
            "pred": " ".join(pred_row)
        })

keys = ["idx","sentence","true","pred"]
with open("mwe_mismatches.csv","w",encoding="utf-8", newline="") as f:
    writer = csv.DictWriter(f, fieldnames=keys)
    writer.writeheader()
    writer.writerows(out)

print(f"Saved {len(out)} mismatches to mwe_mismatches.csv")

# tests the model with random sentences inserted manually
from transformers import pipeline

nlp = pipeline(
    "token-classification",
    model="./mwe_bert",
    tokenizer="./mwe_bert",
    aggregation_strategy="simple"
)

# write any sentence here
sentences = [
    "Mettiamo in pratica un piano ben congeniato per risolvere il problema.",
    "Ha preso una decisione di comune accordo con i colleghi.",
    "Mi vado a fare la doccia.",
    "Il mio modello funziona!",
    "Dimmi una frase qualunque"
]

for sentence in sentences:
    print(f"Sentence: {sentence}")
    predictions = nlp(sentence)
    for pred in predictions:
        print(f"  MWE: {pred['word']}, Score: {pred['score']:.2f}, Start: {pred['start']}, End: {pred['end']}")
    print()